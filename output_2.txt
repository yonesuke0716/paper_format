(2019);アクセントアル。
(2018);アスコリらによる。
（２０２０）。
私たちが研究している小さなアルゴリズム データセットでは、さまざまなモデル、オプティマイザー、およびデータセット サイズで最初のオーバーフィッティングが発生した後の一般化が改善され、場合によってはこれらの効果が非常に顕著です。
モジュール分割の典型的な例を図 1 に示します。
ここでは、トレーニングの精度が最適に近づくために必要な最適化ステップの 1000 倍の回数を超えて初めて、検証の精度が偶然のレベルを超えて増加し始めることがわかります。
図 4 では、トレーニング/検証の損失もプロットされており、検証の損失の二重降下が見られます。
これらの動作は、ネットワークが割り当てられた最適化バジェット内で一般化された最小データセット サイズに近いデータセット サイズのすべてのバイナリ操作で典型的であることがわかりました。
データセットのサイズが大きいほど、トレーニング曲線と検証曲線は互いにより密接に追跡する傾向があります。
３。
１。
1 学習時間曲線典型的な教師あり学習問題では、最適化手順がトレーニング データを補間できる場合、トレーニング データの量を減らすと、モデルの収束した一般化のパフォーマンスが低下します。
私たちの設定では、別の現象が観察されます。トレーニング データセットのサイズの範囲内で、収束したパフォーマンスは 100% で一定の​​ままですが、そのパフォーマンスを達成するために必要な最適化時間は、データセットのサイズが減少するにつれて急速に増加します。
2図 2: 左。
異なる最適化アルゴリズムは、抽象グループ S5 の積を学習する問題の 105 ステップの最適化バジェット内で異なる量の一般化につながります。
重みの減衰は一般化を最も改善しますが、完全なバッチオプティマイザーとトレーニング データの高いパーセンテージでの重みまたはアクティベーション ノイズのないモデルであっても、ある程度の一般化が行われます。
サブ最適選択ハイパーパラメーターは、一般化を大幅に制限します。
表示されていません: すべての最適化方法で 103 ～ 104 回の更新後、トレーニングの精度は 100% に達します。
右。
さまざまなアルゴリズム データセットで 105 ステップ後に達成された最高の検証精度 (3 シードの平均)。
一般化は、直感的により複雑で対称性の低い操作のために、データのより高い割合で発生します。
図 1 (中央) は、抽象グループ S5 の製品の検証パフォーマンスが最初に 99% に達するまでの最適化ステップ数の中央値を示しています。
データの 25 ～ 30% 付近では、トレーニング データが 1% 減少すると、一般化までの時間の中央値が 40 ～ 50% 増加します。
検証精度が 99% を超えるまでのステップ数は、データセットのサイズが減少するにつれて急速に増加しますが、トレーニングの精度が最初に 99% に達するまでのステップ数は、データセットのサイズが減少し、103 ～ 104 の最適化ステップの範囲内に留まるにつれて、一般的に減少傾向にあります。
ネットワークを一般化できるすべてのアルゴリズムタスクでデータセットのサイズが減少するにつれて、一般化に達するまでの最適化時間が指数関数的に増加するという同様のパターンが観察されました。
３。
2 さまざまな問題を理解する 付録 A に記載されているさまざまな 2 項演算で利用可能なすべての方程式のさまざまな部分で構成されるトレーニング データセットについて、3 回の実行で平均精度を測定しました。
１。
１。
結果を図 2 (右) に示します。
オペランドは無関係な抽象記号としてニューラル ネットワークに提示されるため、素数 p とゼロ以外の x、y を持つ操作 x + y (mod p − 1) と x ∗ y (mod p) は、ニューラル ネットワークの観点からは区別できません。 (および同様に x − y (mod p − 1) および x/y (mod p))。
これは、素数を法とするすべての非ゼロ剰余は、原始根の累乗として表すことができるためです。
この表現は剰余加算 modulop − 1 と剰余乗算 modulo p の同等性 (記号の名前変更まで) を示しています。
図 2 (右) を見ると、x − y と x/y が実際に一般化を行うのにほぼ同じ量のデータが必要であることがわかります。
図 2 (右) にリストされている操作の一部は、オペランド (x + y、x ∗ y、x2 + y2、および x2 + xy + y2) の順序に関して対称です。
このような演算は、密接に関連する非対称演算 (x − y、x/y、x2 + xy + y2 + x) よりも一般化に必要なデータが少ない傾向があります。
トランスフォーマーは位置埋め込みを無視することでオペランドの対称関数を簡単に学習できるため、この効果は部分的にアーキテクチャに依存する可能性があると考えています。
一部の操作 (x3 + xy2 + y (mod 97) など) は、最大 95% までのデータの割合で、許可された最適化バジェット内で一般化されませんでした。
収束したモデルは、データ内に実際のパターンを見つけることなく、トレーニング データセットを効果的に記憶しただけです。
このようなモデルでは、データは事実上ランダムです。
[x/y (mod p) y が奇数の場合、それ以外の場合は x − y (mod p)] の操作では、ネットワークがいくつかの単純な操作の組み合わせを学習する必要があります。特に、x の役割は、加法の剰余として解釈する必要があります。偶数の y とペアになっている場合はグループになり、奇数の y とペアになっている場合は乗法群の剰余として。
これは、グループ操作またはリング操作を介して明確に解釈できない操作に対しても一般化が発生する可能性があることを示しています。
