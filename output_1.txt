2202naJ6]GL.
sc[1v77120.
1022:viXraGROCKING: 小さなアルゴリズム データセットでの過適合を超えた一般化Lethea Power、Yuri Burda、Harri Edwards、Igor BabuschkinOpenAIVedant Misra*GoogleABSTRACTこの論文では、小さなアルゴリズムで生成されたデータセットでニューラル ネットワークの一般化を研究することを提案します。
この設定では、データ効率、記憶、一般化、および学習速度に関する質問を非常に詳細に研究できます。
状況によっては、ニューラル ネットワークがデータ内のパターンを「理解」するプロセスを通じて学習し、一般化のパフォーマンスがランダム チャンス レベルから完全な一般化まで改善されること、および一般化のこの改善はオーバーフィッティングのポイントをはるかに超えて発生する可能性があることを示します。
また、データセットのサイズの関数として一般化を研究し、データセットが小さいほど、一般化のために最適化の量を増やす必要があることを発見しました。
これらのデータセットは、深層学習のあまり理解されていない側面を研究するための肥沃な土壌を提供すると主張します。つまり、有限のトレーニングデータセットの記憶を超えた、過剰にパラメーター化されたニューラルネットワークの一般化です。
1はじめに過剰にパラメータ化されたニューラル ネットワークの一般化は、古典的な学習理論から導き出された直感を否定するものであるため、長い間機械学習コミュニティの関心の的でした。
この論文では、アルゴリズムによって生成された小規模なデータセットに対するトレーニング ネットワークが、異常な一般化パターンを確実に示すことができることを示します。これは、トレーニング セットのパフォーマンスから明確に切り離されており、自然データから派生したデータセットに現れるそのような効果よりもはるかに顕著です (図 1 の左を参照)。例）。
このような実験は、単一の GPU ですばやく再現できるため、一般化の理論のテストベッドとして便利です。
図 1: 左。
Grokking: アルゴリズムのデータセットで過剰適合した後の一般化の劇的な例。
トレーニング セットのデータの 50% を使用して、97 を法とする除算のバイナリ演算をトレーニングします。
97 個の残基のそれぞれは、右の図の表現と同様に、個別のシンボルとしてネットワークに提示されます。
赤い曲線はトレーニングの精度を示し、緑の曲線は検証の精度を示しています。
トレーニングの精度は 103 未満の最適化ステップで完璧に近づきますが、検証の精度がそのレベルに到達するには 106 ステップ近くかかり、105 ステップまで一般化の証拠はほとんど見られません。
中心。
99% の検証精度に到達するために必要なトレーニング時間は、トレーニング データの割合が減少するにつれて急速に増加します。
右。
小さなバイナリ操作テーブルの例。
どの要素が欠落しているかについて読者に推測してもらいます。
∗Vedant はこの作業の時点で OpenAI にいました1 私たちが検討するデータセットは、a ◦ b = c の形式の 2 項演算テーブルです。ここで、a、b、c は内部構造を持たない離散記号であり、◦ は 2 項演算です。
二項演算の例としては、加算、順列合成、二変数多項式などがあります。
考えられるすべての方程式の適切なサブセットでニューラル ネットワークをトレーニングすることは、数独パズルを解くのと同じように、2 項演算テーブルの空白を埋めることになります。
例を図 1 の右側に示します。
方程式に含まれるすべての個別の要素 a、b、c に個別の抽象記号を使用したため、ネットワークは要素の内部構造を認識せず、他の要素との相互作用からのみそれらのプロパティを学習する必要があります。
たとえば、ネットワークは 10 進表記の数字や行表記の順列を認識しません。
私たちの貢献は次のとおりです。 • ニューラル ネットワークが、さまざまなバイナリ op テーブルの空のスロットに一般化できることを示します。
• 深刻なオーバーフィッティングの後、検証精度が偶然のレベルから完全な一般化に向かって突然増加し始めることがあることを示します。
この現象を「グロッキング」と呼んでいます。
例を図 1 に示します。
• さまざまなバイナリ操作のデータ効率曲線を提示します。
• データセットのサイズが小さくなると、一般化に必要な最適化の量が急速に増加することが経験的に示されています。
• さまざまな最適化の詳細を比較して、データ効率への影響を測定します。
減量は、研究するタスクの一般化を改善するのに特に効果的であることがわかりました。
• これらのネットワークによって学習されたシンボルの埋め込みを視覚化すると、シンボルによって表される数学的オブジェクトの認識可能な構造が時々明らかになることがわかります。
2 方法私たちのすべての実験では、a ◦ b = c の形式の方程式のデータセットでトレーニングされた小さな変換器を使用しました。ここで、「a」、「◦」、「b」、「=」、および「c」はそれぞれ別のトークンです。 .
調査した操作、アーキテクチャ、トレーニング ハイパーパラメータ、およびトークン化の詳細については、付録 A を参照してください。
１。
3 実験3.
1 オーバーフィッティングを超えた一般化深層学習の実践者は、検証損失が減少しなくなった後、検証精度がわずかに向上することに慣れています。
検証損失の二重降下がいくつかの状況で文書化されていますが、実務家の間では珍しいと考えられています。
